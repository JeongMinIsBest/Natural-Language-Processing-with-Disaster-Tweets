# ğŸš€ Natural Language Processing with Disaster Tweets
This repository contains **Natural Language Processing projects** built with **deep learning language models such as BERT and DistilBERT**.
  
The main projects include:
- **Disaster Tweets Classification** (Kaggle Competition)
- **Sentiment Analysis using NSMC (Naver Sentiment Movie Corpus)**
  
The goal of this repository is to explore **Transformer-based fine-tuning workflows** and compare implementations across **PyTorch and TensorFlow**.
<br/>
<br/>

## ğŸ“‚ Project Overview
  
### 1ï¸âƒ£ Disaster Tweets Classification
This project focuses on **binary text classification**, predicting whether a given tweet refers to a **real disaster event** or not.
The dataset comes from Kaggleâ€™s **â€œNatural Language Processing with Disaster Tweetsâ€** competition.
  
#### ğŸ“ Key Files
- `NLP with Disaster Tweets using DistilBERT-Baseline.ipynb`  
  - PyTorch + Hugging Face Transformers  
  - Baseline DistilBERT fine-tuning pipeline
  
- `KerasNLP_starter_notebook_Disaster_Tweets.ipynb`  
  - TensorFlow + KerasNLP implementation  
  - DistilBERT training and evaluation
  
- `train.csv`, `test.csv`  
  - Training and inference datasets provided by Kaggle
  
#### ğŸ¤– Model
- **DistilBERT** (`distilbert-base-uncased`)
  
#### ğŸ“ˆ Result
- Validation Accuracy: **~0.83** (KerasNLP implementation)
<br/>

### 2ï¸âƒ£ NSMC Sentiment Analysis
This project performs **sentiment classification (positive / negative)** on the **Naver Sentiment Movie Corpus (NSMC)**.
The same task is implemented using **both PyTorch and TensorFlow** to compare model pipelines, training loops, and framework-specific design choices.
  
#### ğŸ“ Key Files
- `nsmc sentiment analysis/nsmc ê°ì„± ë¶„ì„_íŒŒì´í† ì¹˜.ipynb`  
  - PyTorch-based implementation
  
- `nsmc sentiment analysis/nsmc ê°ì„± ë¶„ì„ ê³¼ì œ_í…ì„œí”Œë¡œìš°.ipynb`  
  - TensorFlow (Keras) implementation
  
#### ğŸ¤– Model
- **Multilingual BERT** (`bert-base-multilingual-cased`)
  
#### ğŸ“Š Dataset
- `nsmc` dataset from the **Hugging Face `datasets` library**
<br/>
<br/>

## ğŸ›  Tech Stack
  
- **Languages**
  - Python
  
- **Deep Learning Frameworks**
  - PyTorch
  - TensorFlow / Keras
  
- **NLP Libraries**
  - Hugging Face Transformers
  - KerasNLP
  - Datasets
  
- **Data Processing**
  - Pandas
  - NumPy
  - Scikit-learn
  
- **Visualization**
  - Matplotlib
  - Seaborn
<br/>
<br/>

## ğŸš€ How to Run
All projects are implemented as **Jupyter Notebooks (`.ipynb`)**
- You can run them in:
  - **Google Colab**
  - **Local Jupyter Notebook environments**
  
Each notebook follows the standard NLP workflow:
1. Data loading
2. Preprocessing & tokenization
3. Model fine-tuning
4. Evaluation and analysis
<br/>
<br/>
