{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"18AojOjLfd9cnXiMFydDNMy0I1V40M7qo","timestamp":1543926913861},{"file_id":"1uoOWiCZ8I3qClWEt4N0A0EO0OjOjWZwf","timestamp":1543828054136}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## 선형 회귀\n","### 변수가 두 개인 선형 회귀 모델\n","$y = 1 + 2x_1 +"],"metadata":{"id":"GoZV_o3pFbXU"}},{"cell_type":"markdown","source":["#### (1) 데이터 생성 및 변수 정의"],"metadata":{"id":"JKsUTaIMFnex"}},{"metadata":{"id":"5S2mQl4ePLxW"},"cell_type":"code","source":["import torch\n","\n","# 참의 계수\n","w_true = torch.Tensor([1, 2, 3])\n","\n","# X 데이터 준비. 절편을 회귀 계수에 포함시키기 위해\n","# X의 최초 차원에 1을 추가해둔다\n","X = torch.cat([torch.ones(100, 1), torch.randn(100, 2)], 1)\n","\n","# 참의 게수와 각 X의 내적을 행렬과 벡터의 곱으로 모아서 계산\n","y = torch.mv(X, w_true) + torch.randn(100) * 0.5\n","\n","# 기울기 하강으로 최적화하기 위해 파라미터 Tensor를\n","# 난수로 초기화해서 생성\n","w = torch.randn(3, requires_grad=True)\n","\n","# 학습률\n","gamma = 0.1\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["(2) 파라미터 최적화"],"metadata":{"id":"jObcnDl1Fsdq"}},{"metadata":{"id":"_D1lFTaZP6tN"},"cell_type":"code","source":["# 손실 함수의 로그\n","losses = []\n","\n","# 100회 반복\n","for epoc in range(100):\n","    # 전회의 backward 메서드로 계산된 경사 값을 초기화\n","    w.grad = None\n","\n","    # 선형 모델으로 y 예측 값을 계산\n","    y_pred = torch.mv(X, w)\n","\n","    # MSE loss와 w에 의한 미분을 계산\n","    loss = torch.mean((y - y_pred)**2)\n","    loss.backward()\n","\n","    # 경사를 갱신한다\n","    # w를 그대로 대입해서 갱신하면 다른 Tensor가 돼서\n","    # 계산 그래프가 망가진다. 따라서 data만 갱신한다\n","    w.data = w.data - gamma * w.grad.data\n","\n","    # 수렴 확인을 위한 loss를 기록해둔다\n","    losses.append(loss.item())"],"execution_count":null,"outputs":[]},{"metadata":{"id":"SpnrjozyRD0H"},"cell_type":"code","source":["# loss curve 확인\n","\n","%matplotlib inline\n","from matplotlib import pyplot as plt\n","plt.plot(losses)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"4DfJ9GvoR53b"},"cell_type":"code","source":["w"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 파이토치 모듈을 활용한 선형 회귀 모델"],"metadata":{"id":"gUeDdOCMF6_q"}},{"metadata":{"id":"NdmGrLM_TXQ3"},"cell_type":"code","source":["from torch import nn, optim\n","\n","# Linear층을 작성. 이번에는 절편은 회귀 계수에 포함하므로\n","# 입력 차원을 3으로 하고 bias(절편)을 False로 한다\n","net = nn.Linear(in_features=3, out_features=1, bias=False)\n","\n","# SGD의 optimizer상에서 정의한 네트워크의\n","# 파라미터를 전달해서 초기화\n","optimizer = optim.SGD(net.parameters(), lr=0.1)\n","\n","# MSE loss클래스\n","loss_fn = nn.MSELoss()"],"execution_count":null,"outputs":[]},{"metadata":{"id":"OG9bJhPRVixS"},"cell_type":"code","source":["# 손실 함수 로그\n","losses = []\n","\n","# 100회 반복\n","for epoc in range(100):\n","     # 전회의 backward 메서드로 계산된 경사 값을 초기화\n","    optimizer.zero_grad()\n","\n","    # 선형 모델으로 y 예측 값을 계산\n","    y_pred = net(X)\n","\n","    # MSE loss 계산\n","    # y_pred는 (n,1)과 같은 shape를 지니고 있으므로 (n,)으로 변경할 필요가 있다\n","    loss = loss_fn(y_pred.view_as(y), y)\n","\n","    # loss의 w를 사용한 미분 계산\n","    loss.backward()\n","\n","    # 경사를 갱신한다\n","    optimizer.step()\n","\n","    # 수렴 확인을 위한 loss를 기록해둔다\n","    losses.append(loss.item())"],"execution_count":null,"outputs":[]},{"metadata":{"id":"gB316zRxWpDg"},"cell_type":"code","source":["list(net.parameters())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 로지스틱 회귀"],"metadata":{"id":"r4e3qooWGLJV"}},{"metadata":{"id":"KjFKjw2dcHlh"},"cell_type":"code","source":["from torch import nn, optim\n","from sklearn.datasets import load_iris\n","iris = load_iris()\n","\n","# iris는 (0,1,2)의 세 개 클래스를 분류하는 문제이므로\n","# (0,1)의 두 개 클래스 데이터만 사용한다\n","# 원래는 교육용과 테스트용으로 나누어야 하지만 여기선 생략한다\n","X = iris.data[:100]\n","y = iris.target[:100]\n","\n","# NumPy의 ndarray를 PyTorch의 Tensor로 변환\n","X = torch.tensor(X, dtype=torch.float32)\n","y = torch.tensor(y, dtype=torch.float32)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"3pJRaUxac9id"},"cell_type":"code","source":["# iris 데이터는 4차원\n","net = nn.Linear(4, 1)\n","\n","# 시그모이드 함수를 적용해서 두 클래스의 분류를 위한\n","# 크로스 엔트로피를 계산\n","loss_fn = nn.BCEWithLogitsLoss()\n","\n","# SGD(약간 큰 학습률)\n","optimizer = optim.SGD(net.parameters(), lr=0.25)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"8AxcZzsngoZu"},"cell_type":"code","source":["# 손실 함수 로그\n","losses = []\n","\n","# 100회 반복\n","for epoc in range(100):\n","     # 전회의 backward 메서드로 계산된 경사 값을 초기화\n","    optimizer.zero_grad()\n","\n","    # 선형 모델으로 y 예측 값을 계산\n","    y_pred = net(X)\n","\n","    # MSE loss 미분 계산\n","    loss = loss_fn(y_pred.view_as(y), y)\n","    loss.backward()\n","\n","    # 경사를 갱신한다\n","    optimizer.step()\n","\n","    # 수렴 확인을 위한 loss를 기록해둔다\n","    losses.append(loss.item())\n"],"execution_count":null,"outputs":[]},{"metadata":{"id":"5pv-NUGYgsYM"},"cell_type":"code","source":["%matplotlib inline\n","from matplotlib import pyplot as plt\n","plt.plot(losses)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"i-fC1v5_hQjv"},"cell_type":"code","source":["# 선형 결합의 결과\n","h = net(X)\n","\n","# 시그모이드 함수를 적용한 결과는 y=1의 확률을 보여준다\n","prob = nn.functional.sigmoid(h)\n","\n","# 확률이 0.5이상인 것을 클래스1로 예측하고 그외는 0으로 한다\n","# PyTorch에는 Bool형이 없으므로 ByteTensor가 출력된다.\n","y_pred = prob > 0.5\n","\n","# 예측 결과 확인 (yはFloatTensor이므로 ByteTensor로\n","# 로 변환한 후에 비교）\n","(y.byte() == y_pred.view_as(y)).sum().item()"],"execution_count":null,"outputs":[]},{"metadata":{"id":"Qn_o6W7ti4it"},"cell_type":"code","source":["from sklearn.datasets import load_digits\n","digits = load_digits()\n","\n","X = digits.data\n","y = digits.target\n","\n","X = torch.tensor(X, dtype=torch.float32)\n","# CrossEntropyLoss함수는 y로 int64형의 Tensor를 받으지 주의하자を受け取るので注意\n","y = torch.tensor(y, dtype=torch.int64)\n","\n","# 출력은 10(클래스 수) 차원\n","net = nn.Linear(X.size()[1], 10)\n","\n","# 소프트맥스 크로스 엔트로피\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# SGD\n","optimizer = optim.SGD(net.parameters(), lr=0.01)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"2_CQfpjznnoC"},"cell_type":"code","source":["# 손실 함수 로그\n","losses = []\n","\n","# 100회 반복\n","for epoc in range(100):\n","     # 전회의 backward 메서드로 계산된 경사 값을 초기화\n","    optimizer.zero_grad()\n","\n","    # 선형 모델으로 y 예측 값을 계산\n","    y_pred = net(X)\n","\n","    # MSE loss 미분 계산\n","    loss = loss_fn(y_pred, y)\n","    loss.backward()\n","\n","    # 경사를 갱신한다\n","    optimizer.step()\n","\n","    # 수렴 확인을 위한 loss를 기록해둔다\n","    losses.append(loss.item())\n"],"execution_count":null,"outputs":[]},{"metadata":{"id":"mVIVYmSuo0Xh"},"cell_type":"code","source":["# torch.max눈 집계축을 지정하면 최댓값뿐만 아니라 그 위치도 반환한다\n","_, y_pred = torch.max(net(X), 1)\n","\n","# 정답률을 계산한다\n","(y_pred == y).sum().item() / len(y)"],"execution_count":null,"outputs":[]}]}