{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","metadata":{"id":"VboKwPUKLtQm"},"source":["# PyTorch Basics"]},{"cell_type":"markdown","metadata":{"id":"j0AE_LD9LywH"},"source":["## 모듈 불러오기"]},{"cell_type":"code","metadata":{"id":"pMulwJ5ALkFi"},"source":["import torch\n","import torchvision\n","import torch.nn as nn\n","import numpy as np\n","import torchvision.transforms as transforms\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QRBN4dtOL5sP"},"source":["## Basic autograd example 1"]},{"cell_type":"markdown","metadata":{"id":"hJW2FnJ8MP0h"},"source":["#### Create tensors."]},{"cell_type":"code","metadata":{"id":"A88rkOoyMR1G"},"source":["# Create tensors.\n","x = torch.tensor(1., requires_grad=True)\n","w = torch.tensor(2., requires_grad=True)\n","b = torch.tensor(3., requires_grad=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(x, w, b)"],"metadata":{"id":"5-sJySmF3uqq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EEKgpW-3MMVJ"},"source":["#### Build a computational graph."]},{"cell_type":"markdown","metadata":{"id":"fGwM7KR_O8gj"},"source":["![대체 텍스트](https://i.imgur.com/iUhWEj2.png)"]},{"cell_type":"code","metadata":{"id":"LTQQZksmMHoC"},"source":["y = w * x + b    # y = 2 * x + 3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3x2WNAitTwyA"},"source":[]},{"cell_type":"markdown","metadata":{"id":"ZtnIhNfFMTb6"},"source":["#### Compute gradients."]},{"cell_type":"code","metadata":{"id":"F8LryZt3MW3m"},"source":["# Compute gradients.\n","y.backward()\n","\n","# Print out the gradients.\n","print(x.grad)    # x.grad = 2\n","print(w.grad)    # w.grad = 1\n","print(b.grad)    # b.grad = 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4--omaxeMvc0"},"source":["## 2. Basic autograd example 2"]},{"cell_type":"markdown","metadata":{"id":"bUv9s3gpM0Hz"},"source":["#### (1) Create tensors of shape (10, 3) and (10, 2)."]},{"cell_type":"code","metadata":{"id":"kEZz1jknMusk"},"source":["torch.manual_seed(1102)\n","\n","x = torch.randn(10, 3)\n","y = torch.randn(10, 2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fXXh4dUkNFm2"},"source":["#### 데이터 확인"]},{"cell_type":"code","metadata":{"id":"FM2sVNqqNAQ5"},"source":["print(x)\n","print(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NfcJ6gmSNIle"},"source":["#### (2) Build a fully connected layer.\n","nn.Linear : https://pytorch.org/docs/stable/generated/torch.nn.Linear.html"]},{"cell_type":"code","metadata":{"id":"l52JnwbHNM5g"},"source":["linear = nn.Linear(3, 2)\n","print ('w: ', linear.weight)\n","print ('b: ', linear.bias)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SCSLmnyINgMe"},"source":["#### (3) Build loss function and optimizer."]},{"cell_type":"markdown","metadata":{"id":"8bGL7OK6LN5Z"},"source":["MSE(Mean Squared Error):\n","\n","$ MSE=\\frac{1}{N}\\sum_{i=1}^N (y_i-\\hat{y_i})^2$\n","\n","$y_i$ = 예측값\n","\n","$\\hat{y_i}$ = 실제 정답\n","\n","Q. 왜 제곱을 해서 사용하나요?\n","\n","A. 모든 오차의 양수화, 더 큰 오차를 부각시킴"]},{"cell_type":"code","metadata":{"id":"uUFpc5W5Nj01"},"source":["criterion = nn.MSELoss()\n","optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mk77KdMxNmIu"},"source":["#### (4) Forward pass"]},{"cell_type":"code","metadata":{"id":"rO68jF7fNq9c"},"source":["pred = linear(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fEZeP4K_LyWb"},"source":["print(pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l3WVtM-pNsAT"},"source":["#### (5) Compute loss."]},{"cell_type":"code","metadata":{"id":"_yUuoP4rNzu8"},"source":["loss = criterion(pred, y)\n","print('loss: ', loss.item())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qi0CMir0JXZh"},"source":["np.mean(np.square(pred.detach().numpy()-y.numpy()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nXo4PZVnOBhK"},"source":["#### (6) Backward pass."]},{"cell_type":"code","metadata":{"id":"8ls1nmQCOFZy"},"source":["loss.backward()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xccWGnseOGgO"},"source":["#### Print out the weights and gradients."]},{"cell_type":"code","metadata":{"id":"gUFWDtmmHn2P"},"source":["print ('w: ', linear.weight)\n","print ('b: ', linear.bias)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mP8hhjdyOJ9d"},"source":["print ('dL/dw: ', linear.weight.grad)\n","print ('dL/db: ', linear.bias.grad)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BwuqV7wWOLKJ"},"source":["#### (7) 1-step gradient descent.\n","\n","weight - learning_rate * gradient\n"]},{"cell_type":"code","metadata":{"id":"IJOGfFGhUSlq"},"source":["linear.weight - 0.01 * linear.weight.grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d8No7Cv0OOOu"},"source":["optimizer.step()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gVrxbQ-GVXKE"},"source":["print ('w: ', linear.weight)\n","print ('b: ', linear.bias)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AY4bqEA1OPVb"},"source":["#### Print out the loss after 1-step gradient descent."]},{"cell_type":"code","metadata":{"id":"9K9ONou8OUKx"},"source":["pred = linear(x)\n","loss = criterion(pred, y)\n","print('loss after 1 step optimization: ', loss.item())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 파이토치 딥러닝 학습 기본 구조\n","\n","1. 데이터 로드\n","2. 모델 정의\n","3. Loss와 optimizer 정의\n","4. 학습\n","5. 테스트\n","\n","좋은 예시: https://tutorials.pytorch.kr/beginner/blitz/cifar10_tutorial.html"],"metadata":{"id":"8tVQ00NeJeZL"}}]}